#!/usr/bin/env python3
"""
Example: ML Inference API Server

Demonstrates running the FastAPI ML inference server.
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import uvicorn
from app.api.ml_inference import app


if __name__ == "__main__":
    print(
        """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                           â•‘
â•‘        ğŸš€ ML Inference API Server                                        â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Starting FastAPI server...

ğŸ“ Endpoints:
   â€¢ POST http://localhost:8000/api/ml/predict     - Scan file
   â€¢ GET  http://localhost:8000/api/ml/models      - List models
   â€¢ POST http://localhost:8000/api/ml/models/{v}/reload - Reload model
   â€¢ GET  http://localhost:8000/api/ml/health      - Health check

ğŸ“– Documentation:
   â€¢ Swagger UI: http://localhost:8000/api/ml/docs
   â€¢ ReDoc:      http://localhost:8000/api/ml/redoc
   â€¢ OpenAPI:    http://localhost:8000/api/ml/openapi.json

ğŸ” Authentication:
   â€¢ Set X-API-Key header (if enabled in config)
   â€¢ Default: Authentication disabled for development

âš¡ Rate Limits:
   â€¢ /predict: 10 requests/minute
   â€¢ /models: 30 requests/minute
   â€¢ /health: 30 requests/minute

Starting server on http://0.0.0.0:8000...
    """
    )

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info",
        reload=False,  # Set True for development
    )
